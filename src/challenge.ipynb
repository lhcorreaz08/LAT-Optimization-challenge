{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Memory\n",
    "\n",
    "Estratégia: Delegar el procesamiento de los datos a un microservicio de cloud run de (GCP) asi mismo el almacenamiento del archivo se hace mediante un bucket de GCP. Esto permite enviar unicamente el request con los datos del archivo a procesar y el contenedor de lo aloja.\n",
    "\n",
    "Servicios cloud utilizados (GCP): Cloud Run, Cloud Storage y container registry. \n",
    "\n",
    "La lógica del microservicio se encuentra en el directorio ./Cloud Run/Q1_memory_cloud_run con el uso de flask, la implementación algorithm de map-reduce y el archivo dockerfile para la creación de la imagen.\n",
    "\n",
    "Mejoras: La implementación se puede hacer con un cluster de dataproc para el procesamiento de los datos, esto permitiría el uso de spark para el procesamiento de los datos y el uso de un bucket de GCP para el almacenamiento de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    " \n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    url = 'https://q1memory-qmij3rko2a-ue.a.run.app/q1_memory'\n",
    "    params = {\n",
    "        \"bucket_name\": \"lat_optimization_challenge\",\n",
    "        \"blob_name\": \"data/q1_memory_farmers-protest-tweets-2021-2-4.json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results_raw = response.json()\n",
    "        results = [(datetime.strptime(date_str, '%Y-%m-%d').date(), username) for date_str, username in results_raw]\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Memory\n",
    "\n",
    "Estratégia: Delegar el procesamiento de los datos a un microservicio de cloud run de (GCP) asi mismo el almacenamiento del archivo se hace mediante un bucket de GCP. Esto permite enviar unicamente el request con los datos del archivo a procesar y el contenedor de lo aloja.\n",
    "\n",
    "Servicios cloud utilizados (GCP): Cloud Run, Cloud Storage y container registry.\n",
    "\n",
    "La lógica del microservicio se encuentra en el directorio: ./Cloud Run/Q2_memory_cloud_run con el uso de flask, la implementación algorithm de map-reduce y el archivo dockerfile para la creación de la imagen.\n",
    "\n",
    "Mejoras: La implementación se puede hacer con un cluster de dataproc para el procesamiento de los datos, esto permitiría el uso de spark para el procesamiento de los datos y el uso de un bucket de GCP para el almacenamiento de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    url = 'https://q2memory-qmij3rko2a-ue.a.run.app/q2_memory'\n",
    "    params = {\n",
    "        \"bucket_name\": \"lat_optimization_challenge\",\n",
    "        \"blob_name\": \"data/q2_memory_farmers-protest-tweets-2021-2-4.json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=params)\n",
    "    data = response.json()\n",
    "\n",
    "    result = [(entry[\"emoji\"], entry[\"count\"]) for entry in data]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Memory\n",
    "\n",
    "Estratégia: Delegar el procesamiento de los datos a un microservicio de cloud run de (GCP) asi mismo el almacenamiento del archivo se hace mediante un bucket de GCP. Esto permite enviar unicamente el request con los datos del archivo a procesar y el contenedor de lo aloja.\n",
    "\n",
    "Servicios cloud utilizados (GCP): Cloud Run, Cloud Storage y container registry.\n",
    "\n",
    "La lógica del microservicio se encuentra en el directorio ./Cloud Run/Q3_memory_cloud_run con el uso de flask, la implementación algorithm de map-reduce y el archivo dockerfile para la creación de la imagen.\n",
    "\n",
    "Mejoras: La implementación se puede hacer con un cluster de dataproc para el procesamiento de los datos, esto permitiría el uso de spark para el procesamiento de los datos y el uso de un bucket de GCP para el almacenamiento de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    url = 'https://q3memory-qmij3rko2a-ue.a.run.app/q3_time'\n",
    "    params = {\n",
    "        \"bucket_name\": \"lat_optimization_challenge\",\n",
    "        \"blob_name\": \"data/q3_memory_farmers-protest-tweets-2021-2-4.json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Modificar la estructura de salida para que coincida con el formato requerido\n",
    "    result = [(entry[0], entry[1]) for entry in data]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Time\n",
    "\n",
    "Estratégia: Implementar un algoritmo de map-reduce para el procesamiento de los datos, el cual se encarga de leer el archivo y procesar los datos para obtener el top 10 de usuarios que más tweets han realizado\n",
    "\n",
    "Servicios cloud utilizados (GCP): Ninguno\n",
    "\n",
    "Mejoras: Manejo de librerias como apache spark para el procesamiento de los datos, esto permitiría el uso de spark para el procesamiento de los datos y el uso de un bucket de GCP para el almacenamiento de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "def map_function(line: str) -> Optional[Tuple[datetime.date, str]]:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        fecha = datetime.strptime(tweet['date'][:10], '%Y-%m-%d').date()\n",
    "        usuario = tweet['user']['username']\n",
    "        return fecha, usuario\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decodificando JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "def reduce_function(mapped_values: List[Tuple[datetime.date, str]]) -> List[Tuple[datetime.date, str]]:\n",
    "    tweets_por_fecha = defaultdict(list)\n",
    "    for value in mapped_values:\n",
    "        if value is not None: \n",
    "            fecha, usuario = value\n",
    "            tweets_por_fecha[fecha].append(usuario)\n",
    "\n",
    "    usuarios_top_por_fecha = []\n",
    "    for fecha, usuarios in tweets_por_fecha.items():\n",
    "        conteo_usuarios = Counter(usuarios)\n",
    "        usuario_top = conteo_usuarios.most_common(1)[0][0]\n",
    "        usuarios_top_por_fecha.append((fecha, usuario_top))\n",
    "\n",
    "    return sorted(usuarios_top_por_fecha, key=lambda x: len(tweets_por_fecha[x[0]]), reverse=True)[:10]\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    mapped_values = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            result = map_function(line)\n",
    "            if result is not None:\n",
    "                mapped_values.append(result)\n",
    "\n",
    "    top_fechas = reduce_function(mapped_values)\n",
    "    return top_fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Time\n",
    "\n",
    "Estratégia: Implementar un algoritmo de map-reduce para el procesamiento de los datos, el cual se encarga de leer el archivo y procesar los datos para obtener el top 10 de emojis más utilizados en los tweets\n",
    "\n",
    "Servicios cloud utilizados (GCP): Ninguno\n",
    "\n",
    "Mejoras: Manejo de librerias como apache spark para el procesamiento de los datos, esto permitiría el uso de spark para el procesamiento de los datos y el uso de un bucket de GCP para el almacenamiento de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Función Map: Procesa cada línea (tweet) para encontrar emojis\n",
    "def map_function(line):\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        emojis_found = emoji.emoji_list(tweet['content'])\n",
    "        return [item['emoji'] for item in emojis_found]\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "# Función Shuffle: Organiza los emojis encontrados para su procesamiento en el Reduce\n",
    "def shuffle_and_sort(mapped_values):\n",
    "    all_emojis = []\n",
    "    for emoji_list in mapped_values:\n",
    "        all_emojis.extend(emoji_list)\n",
    "    return Counter(all_emojis)\n",
    "\n",
    "# Función Reduce: Agrega los conteos de cada emoji\n",
    "def reduce_function(shuffled_data):\n",
    "    return shuffled_data.most_common(10)\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mapped_values = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            mapped_values.extend(map_function(line))\n",
    "\n",
    "    shuffled_data = shuffle_and_sort(mapped_values)\n",
    "    reduced_data = reduce_function(shuffled_data)\n",
    "\n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Time\n",
    "\n",
    "Estratégia: Implementar un algoritmo de map-reduce para el procesamiento de los datos, el cual se encarga de leer el archivo y procesar los datos para obtener el top 10 de usuarios más mencionados en los tweets\n",
    "\n",
    "Servicios cloud utilizados (GCP): Ninguno\n",
    "\n",
    "Mejoras: Manejo de librerias como apache spark para el procesamiento de los datos, esto permitiría el uso de spark para el procesamiento de los datos y el uso de un bucket de GCP para el almacenamiento de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def map_mentions(file_path: str) -> List[Tuple[str, int]]:\n",
    "    user_mentions_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for linea in file:\n",
    "            try:\n",
    "                tweet = json.loads(linea)\n",
    "                user_mentions = re.findall(r'@(\\w+)', tweet['content'])\n",
    "                user_mentions_counter.update(user_mentions)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decodificando JSON: {e}\")\n",
    "\n",
    "    return user_mentions_counter.items()\n",
    "\n",
    "def reduce_mentions(mapped_mentions: List[Tuple[str, int]]) -> List[Tuple[str, int]]:\n",
    "    # Reducir los recuentos sumando los valores para cada usuario\n",
    "    user_mentions_counter = Counter()\n",
    "    for user, count in mapped_mentions:\n",
    "        user_mentions_counter[user] += count\n",
    "\n",
    "    # Obtener los top 10 usuarios más mencionados\n",
    "    top_user_mentions = user_mentions_counter.most_common(10)\n",
    "\n",
    "    return top_user_mentions\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mapped_mentions = map_mentions(file_path)\n",
    "    top_user_mentions = reduce_mentions(mapped_mentions)\n",
    "\n",
    "    # Adaptar la estructura de salida\n",
    "    adapted_output = [(user, count) for user, count in top_user_mentions]\n",
    "\n",
    "    return adapted_output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
